# 0.1.1 release

- Expanded Inference parameters to allow for more LLM behavior configuration
- Added Memory parameters to allow LLM behavior configuration when working with files