# 0.1.9 release

**Perf improvements**
- (LLamaSharp) migrate from ChatSession to Conversation and BatchExecutor
- Fixes for KernelMemory, now memory is properly disposed
- New example (conversation agent)
- CLI release (big perf improvement for infer)
- Allow to enable or disable cache for model weights
- Allow to disable buildin llama.cpp logs and MaIN notification